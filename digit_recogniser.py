# -*- coding: utf-8 -*-
"""digit recogniser.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NhIXzzVgk7-63LO0Buq6YCk_C43iHDly
"""

import pandas as pd
import numpy as np
import cv2
from tqdm import tqdm
import os
import csv

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# GET BEST PARAMS

os.chdir('/content/drive/My Drive/Digit Recognizer/Parameters')
params = {}
for l in range(1,4):
    params['W' + str(l)] = np.array(pd.read_csv('params W' + str(l) + '.csv').values)
    params['b' + str(l)] = np.array(pd.read_csv('params b' + str(l) + '.csv').values)

def create_op_vec(i):
    op = np.zeros((10,1))
    op[i] = 1
    return op

# loading data
os.chdir('/content/drive/My Drive/Digit Recognizer')
train_csv = pd.read_csv('train.csv')
data = np.array(train_csv.values)

# gathering data
Y = data.T[0].T
X = np.uint8(data.T[1:785].T)

# generating test and train sets
x_train = X[0:40000]
y_train = Y[0:40000]
x_test = X[40000:42000]
y_test = Y[40000:42000]

for i in range(X.shape[0]):
    _, f = cv2.threshold(X[i].reshape((28,28)), 80, 255, cv2.THRESH_BINARY)
    X[i] = f.reshape(784)
    X[i] = X[i]/255

Y_train = []
for i in y_train:
    Y_train.append(create_op_vec(int(i)))
Y_train = np.array(Y_train)
Y_train = Y_train.reshape(40000, 10)
Y_train = Y_train.T

Y_test = []
for i in y_test:
    Y_test.append(create_op_vec(int(i)))
Y_test = np.array(Y_test)
Y_test = Y_test.reshape(2000, 10)
Y_test = Y_test.T

# cost function

def cost_function(parameters, A, Y, lambd):
    m = Y.shape[1]
    F = Y*np.log(A) + (1-Y)*np.log(1-A)
    J = -np.sum(np.sum(F, axis = 1))/m
    parasum = 0
    L = int(len(parameters)/2)
    for l in range(1, L + 1):
        parasum += np.sum(np.sum(parameters['W' + str(l)]**2))
    return J + parasum*lambd/(2*m)

# FORWARD PROPAGATION

def initialise_parameters(layer_dims):
    L = len(layer_dims) - 1
    
    parameters = {}
    
    for l in range(1,L + 1):
        parameters["W" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01
        parameters["b" + str(l)] = np.random.randn(layer_dims[l], 1) * 0.01
        
        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))
        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))
        
    return parameters

def relu(Z):
    return np.maximum(0, Z)

def sigmoid(Z):
    return 1/(1 + np.exp(-Z))

def linear_jump(A_prev, W, b):
    return np.dot(W, A_prev) + b

def forward_prop(parameters, X, Y, i, lambd):
    
    L = int(len(parameters)/2)
    cacheZ = []
    cacheA = []
    cacheA.append(X)
    A_prev = X
    
    for l in range(1,L):
        Z = linear_jump(A_prev, parameters["W" + str(l)], parameters["b" + str(l)])
        A_prev = relu(Z)
        cacheZ.append(Z)
        cacheA.append(A_prev)
    
    Z_final = linear_jump(A_prev, parameters["W" + str(L)], parameters["b" + str(L)])
    AL = sigmoid(Z_final)
    
    cacheZ.append(Z_final)
    
    
    if i%5 == 0:
        print("Cost at iteration " + str(i) + " = " + str(cost_function(parameters, AL, Y, lambd)))
    
    return cacheA, cacheZ, AL

# BACKWARD PROPAGATAION

def relu_backward(Z):
    final = np.maximum(0, Z)
    for i in range(final.shape[0]):
        for j in range(final.shape[1]):
            if final[i][j] != 0:
                final[i][j] = 1
    
    return final

def sigmoid_backward(Z):
    A = sigmoid(Z)
    return A*(1-A)

def back_prop(parameters, X, Y, iteration, lambd):
    
    m = X.shape[1]
    L = int(len(parameters)/2)
    cacheA, cacheZ, AL = forward_prop(parameters, X, Y, iteration, lambd)
    gradients = {}
    
    dZ = AL - Y
    gradients['dW' + str(L)] = np.dot(dZ, cacheA[L-1].T)/m + lambd * parameters["W" + str(L)] / m
    gradients['db' + str(L)] = np.sum(dZ, axis=1, keepdims=True)/m
    dA = np.dot(parameters['W' + str(L)].T, dZ)
    
    for l in reversed(range(1,L)):
        dZ = dA * relu_backward(cacheZ[l-1])
        gradients['dW' + str(l)] = np.dot(dZ, cacheA[l-1].T)/m + lambd * parameters["W" + str(l)] / m
        gradients['db' + str(l)] = np.sum(dZ, axis=1, keepdims=True)/m
        dA = np.dot(parameters['W' + str(l)].T, dZ)
        
    return gradients

# UPDATING PARAMETERS

def jump(parameters, gradients, alpha):
    L = int(len(parameters)/2)
    for l in range(1, L + 1):
        parameters["W" + str(l)] -= alpha*gradients["dW" + str(l)]
        parameters["b" + str(l)] -= alpha*gradients["db" + str(l)]
        
    
    return parameters

# TRAINING OUR MODEL

def train(X, Y, alpha, iterations, layer_dims, parameters, lambd):
        
    for i in tqdm(range(iterations)):
        gradients = back_prop(parameters, X, Y, i, lambd)
        parameters = jump(parameters, gradients, alpha)
    
    return parameters

# SPECIFYING NEURAL NETWORK

n_x = 784
n_y = 10
n_h1 = 500
n_h2 = 500
n_h3 = 500
layer_dims = np.array([n_x, n_h1, n_h2, n_y])
r_parameters = initialise_parameters(layer_dims)

# GETTING OPTIMAL PARAMETERS

params = train(x_train.T, Y_train, 0.102, 1000, layer_dims, params, 0)

params

_, _, A = forward_prop(params, x_test.T, Y_test, 0, 0)
_, _, A_t = forward_prop(params, x_train.T, Y_train, 0, 0)

pred = np.argmax(A, axis=0)
pred1 = np.argmax(A_t, axis=0)

np.mean(y_test == pred)

np.mean(y_train == pred1)

x_train.shape

for i in range(len(pred)):
    print(str(pred[i]) + " " + str(y_test[i]))

y_test.tolist()

# SAVING THE PARAMETERS IN A CSV FILE

os.chdir('/content/drive/My Drive/Digit Recognizer/Parameters/')
L = int(len(params)/2)
for l in tqdm(range(1, L+1)):
    with open('params W' + str(l) + '.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(np.ones(params['W' + str(l)].shape[1]) * 2)
        writer.writerows(params['W' + str(l)])
    
    with open('params b' + str(l) + '.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(np.ones(params['b' + str(l)].shape[1]) * 2)
        writer.writerows(params['b' + str(l)])

